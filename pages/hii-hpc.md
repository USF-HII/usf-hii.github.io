---
layout: page
---

<img src="/images/usf-hii-logo.png" border="0" width="30%" height="30%" />
<br/>

---

**Note: Please disregard the USF Research Computing message regarding quotas - HII maintains separate filesystems with enhanced quotas.**

---

## HII-HPC Cluster

In partnership with [USF Research Computing](http://www.usf.edu/it/research-computing/),
the [Health Informatics Institute](http://www.hii.usf.edu)
offers the HII-HPC Cluster for its faculty and partners requiring
large-scale computational resources for bioinformatics workloads.

### Latest

- [High Memory Nodes](hii-hpc/himem-nodes.html)

### Overview

- [Purpose](hii-hpc/purpose.html)
- [Linux](hii-hpc/linux.html)
- [Connecting](hii-hpc/connect.html)
- [Filesystems](hii-hpc/filesystems.html)
- [Slurm](hii-hpc/slurm.html)
- [Modules](hii-hpc/modules.html)
- [Getting Help](hii-hpc/help.html)
- [Frequently Asked Questions](hii-hpc/faq.html)
- [Other Topics](hii-hpc/other.html)

### Availability

The HII-HPC Cluster schedules 2 maintenance windows on the same day each week:

- Change Window A: Head Nodes: Each Thursday @ 10:00 AM EDT until 11:00 AM EDT
- Change Window B: Head Nodes/Compute Nodes: Each Thursday @ 10:00 PM EDT until 06:00 AM EDT

The Change Window A is for updates to the Head Nodes (e.g. `hii.rc.usf.edu`) but currently running jobs will not be affected.

The Change Window B is for updates to Head Nodes and the Compute Nodes.

**Note:** *Although Change Window B events are less common, please
verify your job status after Change Window B if you are running jobs utilizing Compute Nodes and re-submit as necessary.*

### News

**2016**

- 2 Nodes: 28 Xeon E5-2650 v4 @ 2.30GHz cores with 1024 GB RAM @ 2400 MHz ([High Memory Nodes](hii-hpc/himem-nodes.html))
- 40 Nodes: 20 Xeon E5-2650 v3 @ 2.30GHz cores with 128 GB RAM @ 2133 MHz

**2015**

- 40 Nodes: 16 Xeon E5-2650 v2 @ 2.60GHz with 128 GB RAM @ 1600 MHz
- DDN General Parallel File System (GPFS) Storage Cluster providing Petabytes of scalable I/O

